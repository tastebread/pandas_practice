import pandas as pd
DataUrl = 'https://raw.githubusercontent.com/Datamanim/pandas/main/chipo.csv'
df = pd.read_csv(DataUrl)
Ans = type(df)


# 21번 - > quantity컬럼 값이 3인 데이터를 추출하여 첫 5행을 출력
df.loc[df['quantity'] == 3].head(5)
# 22번 - > quantity컬럼 값이 3인 데이터를 추출하여 index를 0부터 정렬하고 첫 5행을 출력
#reset_index -> index를 0부터 다시 정렬함
df.loc[df['quantity'] == 3].head(5).reset_index(drop=True)
# 23번 - > quantity, item_price 두개의 컬럼으로 구성된 새로운 데이터 프레임을 정의하라
df3 = df[['quantity','item_price']]
# 24번 - > item_price 컬럼의 달러표시 문자를 제거하고 float 타입으로 저장하여 new_price 컬럼에 저장
new_col = df['item_price'].str[1:].astype(float)
df['new_price'] = new_col
df['new_price'].head()

#25번 -> new_price 컬럼이 5이하의 값을 가지는 데이터프레임을 추출하고 전체 갯수를 구하여라
len(df.loc[df['new_price'] <= 5])

#26번 -> item_name명이 chicken Salad Bowl 인 데이터 프레임을 추출하고 index 값을 초기화하여라
df[df['item_name'] == 'Chicken Salad Bowl'].reset_index().head(5)

#27번 - > new_price 값이 9이하이고 item_name 값이 chicken salad bowl 인 데이터 프레임을 추출하라
#조건을 여러개할때는 앞에  ( 조건쓸내용) 을 붙여준다
df.loc[(df['new_price'] <= 9) & (df['item_name'] == 'Chicken Salad Bowl')].head(5)

#28번 -> df의 new_price 컬럼 값에 따라 오름차순으로 정리하고 index를 초기화 하여라
#오름 차순으로 정렬할때 sort_values 사용 ascending=True -> 오름차순(디폴트값) False -> 내림차순
df.sort_values('new_price').reset_index()

#29번 -> df의 item_name 컬럼 값중 Chips 포함하는 경우의 데이터를 출력하라
#특정문자열을 포함하는 행 추출하는 함수 -> str.contains('포함되는 문자열')
df.loc[df['item_name'].str.contains('Chips')]

#30번 -> df의 짝수번째 컬럼만을 포함하는 데이터프레임을 출력하라
#iloc ->행이나 칼럼의 순서를 나타내는 정수로 특정 값을 추출해오는 방법이다
#loc  -> 칼럼명을 직접 적거나 특정 조건식을 써줌으로써 사람이 읽기 좋은 방법
df.iloc[: ,::2].head() #ex) : 모든행 추출, 모든컬럼추출 : 2 짝수만출력
# df.ilc[시작행:끝나는행, 시작하는컬럼:끝나는컬럼:몇단위로 끝어서 출력하는 값]

#31번 -> df의 new_price 컬럼 값에 따라 내림차순으로 정리하고 index를 초기화 하여라
df.sort_values('new_price', ascending=False).reset_index().head(5)

#32번 -> df의 item_name 컬럼 값이 Steak Salad 또는 Bowl인 데이터를 인덱싱 하라
df.loc[(df['item_name'] == 'Steak Salad') | (df['item_name'] == 'Bowl')].head(5)

#33번 -> df의 item_name 컬럼 값이 Steak Salad 또는 Bowl 인 데이터를 데이터 프레임화 한 후, 
#item_name를 기준으로 중복행이 있으면 제거하되 첫번째 케이스만 남겨라
df_33 = df.loc[(df['item_name'] == 'Steak Salad') | (df['item_name'] == 'Bowl')]
#drop_duplicates -> 중복행제거 함수 아무것도적지 않으면 모든 행이 전부 일치하는 대상으로 제거
#일부 열의 값만 중복되는경우 삭제하고싶다면 subset 메서드를 쓰면됨
df_33 = df_33.drop_duplicates(subset = ['item_name'])
df_33

#34번 -> df의 item_name 컬럼 값이 Steak Salad 또는 Bowl 인 데이터를 데이터 프레임화 한 후, 
#item_name를 기준으로 중복행이 있으면 제거하되 마지막 케이스만 남겨라
df_33 = df_33.drop_duplicates(subset = ['item_name'], keep='last')
#마지막케이스만 남기고싶은경우 keep='last' 사용 , 첫번째케이스는 안써도 상관없음
df_33

#35번 -> df의 데이터 중 new_price값이 new_price값의 평균값 이상을 가지는 데이터들을 인덱싱하라
new_price_middle = df['new_price'].mean() #new_price 칼럼의 평균값 찾기
new_price_middle
df.loc[df['new_price'] > new_price_middle]

#36번 -> df의 데이터 중 item_name의 값이 Izze 데이터를 Fizzy Lizzy로 수정하라
df.loc[df.item_name == 'Izze','item_name'] = 'Fizzy Lizzy'

#37번 -> df의 데이터 중 choice_description 값이 NaN 인 데이터의 갯수를 구하여라
#NaN 데이터 갯수를 찾는 함수 .isnull() , sum() -> 데이터 갯수 총합
df['choice_description'].isnull().sum()

#38번 -> df의 데이터 중 choice_description 값이 NaN 인 데이터를 NoData 값으로 대체하라(loc 이용)
df.loc[df.choice_description.isnull(),'choice_description'] = 'NoData'
#choice_description 컬럼안에 Nan 데이터값을 NoData로 변경
df_38 = df
df_38.head()
#39번 -> df의 데이터 중 choice_description 값에 Black이 들어가는 경우를 인덱싱하라
df.loc[df.choice_description.str.contains('Black')].head(5)
#40번 -> df의 데이터 중 choice_description 값에 Vegetables 들어가지 않는 경우의 갯수를 출력하라
#반전연산자인 invert,~를 사용하여 문제를 푼다
df_40 = len(df.loc[~df.choice_description.str.contains('Vegetables')])
df_40

#41번 -> df의 데이터 중 item_name 값이 N으로 시작하는 데이터를 모두 추출하라
#Series.str.startswith(), Series.str.endswith()는 특정 문자열로 시작되거나 끝나는 요소를 찾아 줍니다

df.loc[df['item_name'].str.startswith('N')].head(3)

#42번 -> df의 데이터 중 item_name 값의 단어갯수가 15개 이상인 데이터를 인덱싱하라
df.loc[df['item_name'].str.len() >=15].head(3) # .str.len() -> 각 요소의 길이를 반환하는 함수

#43번 -> df의 데이터 중 new_price값이 lst에 해당하는 경우의 데이터 프레임을 구하고 그 갯수를 출력하라 
#lst =[1.69, 2.39, 3.39, 4.45, 9.25, 10.98, 11.75, 16.98]

#.isin -> 데이터프레임 컬럼에서 어떤 list의 값을 포함하고 있는것만 걸러낼때 사용하는 함수
lst =[1.69, 2.39, 3.39, 4.45, 9.25, 10.98, 11.75, 16.98]

df_43 = df.loc[df.new_price.isin(lst)]
len(df_43)
